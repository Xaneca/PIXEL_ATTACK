# PIXEL_ATTACK
This repository explores adversarial attacks that challenge the robustness of image classification models. Our goal is to design and evaluate algorithms capable of identifying minimal pixel perturbations that successfully mislead deep learning classifiers.

# Pixel-Attacks Project README

This README describes the folder structure and the functionality of the main files in the **PIXEL-ATTACKS** project, including metric scripts, graph generation, and distortion analysis.

---

## ğŸ“„ Main Files

### ğŸ“Š **Metric Scripts â€” `PIXEL-ATTACKS/METRICS FILES/`**

#### `results_analysis.py`

Extracts and analyses key metrics such as:

* **Covered pixels**
* **Adversarial images found**
* **Evaluations until an adversarial pixel is discovered**
* **Success rate**
* **Distortion**
* **Execution time**

  * Uses the `time_file` in append mode (`'a'`)
  * Currently set to retrieve only the last `nruns`

**Outputs produced:**

* `results/results_full.txt` â€” full information: all runs, means, and standard deviations
* `results/results_simp.txt` â€” simplified version: means and standard deviations per approach

---

### ğŸ–¼ï¸ **Image & Graph Generation â€” `make_images.py`**

Runs multiple approaches and models, producing graphs over evolutionary generations.

**Outputs stored in:** `results/graphics/`

**Generated charts:**

* `{modelName}_acc.png` â€” accumulated adversarial images
* `{modelName}_adv.png` â€” adversarial images per generation
* `{modelName}_fit_avg.png` â€” average fitness per generation
* `{modelName}_fit_best.png` â€” best fitness per generation
* `{modelName}_fit.png` â€” combined fitness graph

---

## ğŸ” Distortion Measurement

Distortion quantifies the difference between an adversarial image and its original version.

Since only one pixel is changed, it is computed as:

```
dist = |IR âˆ’ PR| + |IG âˆ’ PG| + |IB âˆ’ PB|
```

Where `IR, IG, IB` are the original pixel values and `PR, PG, PB` are the perturbed values.

### Scripts involved:

#### `CountDif_orig_img.py`

* Calculates the distortion of each successful adversarial pixel
* Inputs:

  * `./results/{modelName}/metrics_mean_img.csv` â€” image index
  * `./results/{modelName}/{approach}/run_{i_run}/img_{i_img}/success_file.csv` â€” genotype of successful pixels

#### `StatsDif.py`

* Computes the average distortion per run
* Input: `./results/{modelName}/`
* Output: `./results/{modelName}/run/img/new_success_file.csv`

#### `medias_dif_per_model.py`

* Computes the average distortion per model
* Input: `./results/{modelName}/run/img/new_success_file.csv`
* Output: `./results/{modelName}/difMeans.csv`

---

## ğŸ” Local Search â€” `local_search.py`

After adversarial pixels are initially detected, this script searches around those pixels to find alternatives that may be:

* more effective
* less perceptible

---

If you want, I can also add installation instructions, examples of usage, or GitHub-style formatting like badges and tables.
