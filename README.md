# PIXEL_ATTACK
This repository explores adversarial attacks that challenge the robustness of image classification models. Our goal is to design and evaluate algorithms capable of identifying minimal pixel perturbations that successfully mislead deep learning classifiers.
